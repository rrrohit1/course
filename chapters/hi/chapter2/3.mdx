<FrameworkSwitchCourse {fw} />

# मॉडल्स 

{#if fw === 'pt'}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter2/section3_pt.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter2/section3_pt.ipynb"},
]} />

{:else}

<DocNotebookDropdown
  classNames="absolute z-10 right-0 top-0"
  options={[
    {label: "Google Colab", value: "https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/chapter2/section3_tf.ipynb"},
    {label: "Aws Studio", value: "https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/chapter2/section3_tf.ipynb"},
]} />

{/if}

{#if fw === 'pt'}
<Youtube id="AhChOFRegn4"/>
{:else}
<Youtube id="d3JVgghSOew"/>
{/if}

{#if fw === 'pt'}
इस खंड में हम एक मॉडल बनाने और उसका उपयोग करने पर करीब से नज़र डालेंगे। हम `AutoModel` वर्ग का उपयोग करेंगे, जो तब काम आता है जब आप किसी चेकपॉइंट से किसी मॉडल को इंस्टेंट करना चाहते हैं।

`AutoModel` वर्ग और उसके सभी रिश्तेदार वास्तव में पुस्तकालय में उपलब्ध विभिन्न प्रकार के मॉडलों पर सरल आवरण हैं। यह एक चतुर आवरण है क्योंकि यह स्वचालित रूप से आपके चेकपॉइंट के लिए उपयुक्त मॉडल आर्किटेक्चर का अनुमान लगा सकता है, और फिर इस आर्किटेक्चर के साथ एक मॉडल को इंस्टेंट करता है।

{:else}
इस खंड में हम एक मॉडल बनाने और उसका उपयोग करने पर करीब से नज़र डालेंगे। हम `TFAutoModel` वर्ग का उपयोग करेंगे, जो तब काम आता है जब आप किसी चेकपॉइंट से किसी मॉडल को इंस्टेंट करना चाहते हैं।

`TFAutoModel` वर्ग और उसके सभी रिश्तेदार वास्तव में पुस्तकालय में उपलब्ध विभिन्न प्रकार के मॉडलों पर सरल आवरण हैं। यह एक चतुर आवरण है क्योंकि यह स्वचालित रूप से आपके चेकपॉइंट के लिए उपयुक्त मॉडल आर्किटेक्चर का अनुमान लगा सकता है, और फिर इस आर्किटेक्चर के साथ एक मॉडल को इंस्टेंट करता है।

{/if}

हालाँकि, यदि आप जानते हैं कि आप किस प्रकार के मॉडल का उपयोग करना चाहते हैं, तो आप उस वर्ग का उपयोग कर सकते हैं जो इसके आर्किटेक्चर को सीधे परिभाषित करता है। आइए देखें कि यह BERT मॉडल के साथ कैसे काम करता है।

## एक ट्रांसफार्मर बनाना

BERT मॉडल को इनिशियलाइज़ करने के लिए सबसे पहले हमें एक कॉन्फ़िगरेशन ऑब्जेक्ट लोड करना होगा:

{#if fw === 'pt'}
```py
from transformers import BertConfig, BertModel

# Building the config
config = BertConfig()

# Building the model from the config
model = BertModel(config)
```
{:else}
```py
from transformers import BertConfig, TFBertModel

# Building the config
config = BertConfig()

# Building the model from the config
model = TFBertModel(config)
```
{/if}

कॉन्फ़िगरेशन में कई विशेषताएँ होती हैं जिनका उपयोग मॉडल बनाने के लिए किया जाता है:

```py
print(config)
```

```python out
BertConfig {
  [...]
  "hidden_size": 768,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  [...]
}
```

जबकि आपने यह नहीं देखा है कि ये सभी विशेषताएँ अभी तक क्या करती हैं, आपको उनमें से कुछ को पहचानना चाहिए: `hidden_size` विशेषता `hidden_states` वेक्टर के आकार को परिभाषित करती है, और `num_hidden_layers` ट्रांसफ़ॉर्मर मॉडल की परतों की संख्या को परिभाषित करती है।

### विभिन्न लोडिंग तरीके

डिफ़ॉल्ट कॉन्फ़िगरेशन से एक मॉडल बनाना इसे यादृच्छिक मानों के साथ आरंभ करता है:

{#if fw === 'pt'}
```py
from transformers import BertConfig, BertModel

config = BertConfig()
model = BertModel(config)

# Model is randomly initialized!
```
{:else}
```py
from transformers import BertConfig, TFBertModel

config = BertConfig()
model = TFBertModel(config)

# Model is randomly initialized!
```
{/if}

इस स्थिति में मॉडल का उपयोग किया जा सकता है, लेकिन यह अस्पष्ट आउटपुट देगा; इसे पहले प्रशिक्षित करने की जरूरत है। हम काम पर मॉडल को शुरुआत से प्रशिक्षित कर सकते हैं, लेकिन जैसा कि आपने [अध्याय 1](/course/chapter1) में देखा, इसके लिए एक लंबे समय और बहुत सारे डेटा की आवश्यकता होगी, और इसमें एक गैर-नगण्य पर्यावरण होगा प्रभाव। अनावश्यक और दोहराए गए प्रयासों से बचने के लिए, उन मॉडलों को साझा करने और पुन: उपयोग करने में सक्षम होना अनिवार्य है जिन्हें पहले ही प्रशिक्षित किया जा चुका है।

पहले से प्रशिक्षित ट्रांसफॉर्मर मॉडल को लोड करना सरल है - हम इसे `from_pretrained()` विधि का उपयोग करके कर सकते हैं:

{#if fw === 'pt'}
```py
from transformers import BertModel

model = BertModel.from_pretrained("bert-base-cased")
```

जैसा कि आपने पहले देखा, हम `BertModel` को समतुल्य `AutoModel` वर्ग से बदल सकते हैं। हम इसे अभी से करेंगे क्योंकि यह चेकपॉइंट-अज्ञेय कोड उत्पन्न करता है; यदि आपका कोड एक चेकपॉइंट के लिए काम करता है, तो उसे दूसरे चेकपॉइंट के साथ निर्बाध रूप से काम करना चाहिए। यह तब भी लागू होता है जब आर्किटेक्चर अलग होता है, जब तक चेकपॉइंट को एक समान कार्य के लिए प्रशिक्षित किया जाता है (उदाहरण के लिए, एक भावना विश्लेषण कार्य)।

{:else}
```py
from transformers import TFBertModel

model = TFBertModel.from_pretrained("bert-base-cased")
```

जैसा कि आपने पहले देखा, हम `TFBertModel` को समतुल्य `TFAutoModel` वर्ग से बदल सकते हैं। हम इसे अभी से करेंगे क्योंकि यह चेकपॉइंट-अज्ञेय कोड उत्पन्न करता है; यदि आपका कोड एक चेकपॉइंट के लिए काम करता है, तो उसे दूसरे चेकपॉइंट के साथ निर्बाध रूप से काम करना चाहिए। यह तब भी लागू होता है जब आर्किटेक्चर अलग होता है, जब तक चेकपॉइंट को एक समान कार्य के लिए प्रशिक्षित किया जाता है (उदाहरण के लिए, एक भावना विश्लेषण कार्य)।

{/if}

उपरोक्त कोड नमूने में हमने `BertConfig` का उपयोग नहीं किया, और इसके बजाय `bert-base-cased` पहचानकर्ता के माध्यम से एक पूर्व-प्रशिक्षित मॉडल लोड किया। यह एक मॉडल चेकपॉइंट है जिसे स्वयं BERT के लेखकों द्वारा प्रशिक्षित किया गया था; आप इसके बारे में [model card](https://huggingface.co/bert-base-cased) में अधिक जानकारी प्राप्त कर सकते हैं।

यह मॉडल अब चेकपॉइंट के सभी भारों के साथ आरंभ किया गया है। इसका उपयोग सीधे उन कार्यों के अनुमान के लिए किया जा सकता है जिन पर इसे प्रशिक्षित किया गया था, और इसे एक नए कार्य पर भी ठीक किया जा सकता है। शुरुआत के बजाय पहले से प्रशिक्षित वजन के साथ प्रशिक्षण से, हम जल्दी से अच्छे परिणाम प्राप्त कर सकते हैं।

वेट को डाउनलोड और कैश किया गया है (इसलिए भविष्य में `from_pretrained()` विधि को कॉल करने के लिए उन्हें फिर से डाउनलोड नहीं करेंगे) कैश फ़ोल्डर में, जो डिफ़ॉल्ट रूप से *~/.cache/huggingface/transformers* के लिए है। आप `HF_HOME` पर्यावरण वेरिएबल सेट करके अपने कैशे फ़ोल्डर को अनुकूलित कर सकते हैं।

मॉडल को लोड करने के लिए उपयोग किया जाने वाला पहचानकर्ता मॉडल हब पर किसी भी मॉडल का पहचानकर्ता हो सकता है, जब तक कि यह BERT आर्किटेक्चर के अनुकूल हो। उपलब्ध BERT चेकपॉइंट्स की पूरी सूची [यहां](https://huggingface.co/models?filter=bert) पाई जा सकती है।

### सेविंग के तरीके

एक मॉडल को सहेजना उतना ही आसान है जितना कि एक को लोड करना - हम `save_pretrained()` विधि का उपयोग करते हैं, जो `from_pretrained()` विधि के अनुरूप है:

```py
model.save_pretrained("directory_on_my_computer")
```

This saves two files to your disk:

{#if fw === 'pt'}
```
ls directory_on_my_computer

config.json pytorch_model.bin
```
{:else}
```
ls directory_on_my_computer

config.json tf_model.h5
```
{/if}

यदि आप *config.json* फ़ाइल पर एक नज़र डालते हैं, तो आप मॉडल आर्किटेक्चर के निर्माण के लिए आवश्यक विशेषताओं को पहचान लेंगे। इस फ़ाइल में कुछ मेटाडेटा भी शामिल हैं, जैसे कि चेकपॉइंट कहाँ से उत्पन्न हुआ और 🤗 ट्रांसफ़ॉर्मर संस्करण जिसका उपयोग आप पिछली बार चेकपॉइंट को सहेजते समय कर रहे थे।

{#if fw === 'pt'}
*pytorch_model.bin* फाइल को *state dictionary* के रूप में जाना जाता है; इसमें आपके मॉडल के सभी भार शामिल हैं। दो फाइलें एक साथ रहती हैं; आपके मॉडल के आर्किटेक्चर को जानने के लिए कॉन्फ़िगरेशन आवश्यक है, जबकि मॉडल वज़न आपके मॉडल के पैरामीटर हैं।

{:else}
*tf_model.h5* फाइल को *state dictionary* के रूप में जाना जाता है; इसमें आपके मॉडल के सभी भार शामिल हैं। दो फाइलें एक साथ रहती हैं; आपके मॉडल के आर्किटेक्चर को जानने के लिए कॉन्फ़िगरेशन आवश्यक है, जबकि मॉडल वज़न आपके मॉडल के पैरामीटर हैं।

{/if}

## अनुमान के लिए ट्रांसफॉर्मर मॉडल का उपयोग करना

अब जब आप जानते हैं कि किसी मॉडल को कैसे लोड और सहेजना है, तो आइए कुछ भविष्यवाणियां करने के लिए इसका उपयोग करने का प्रयास करें। ट्रांसफॉर्मर मॉडल केवल संख्याओं को संसाधित कर सकते हैं - संख्याएं जो टोकननाइज़र उत्पन्न करती हैं। लेकिन इससे पहले कि हम टोकनर्स पर चर्चा करें, आइए देखें कि मॉडल किन इनपुट्स को स्वीकार करता है।

टोकेनाइज़र इनपुट को उपयुक्त फ्रेमवर्क के टेंसर में डालने का ध्यान रख सकते हैं, लेकिन यह समझने में आपकी मदद करने के लिए कि क्या हो रहा है, हम इस पर एक झटपट नज़र डालेंगे कि मॉडल को इनपुट भेजने से पहले क्या किया जाना चाहिए।

मान लीजिए कि हमारे पास कुछ क्रम हैं:

```py
sequences = ["Hello!", "Cool.", "Nice!"]
```

टोकननाइज़र इन्हें शब्दावली सूचकांकों में परिवर्तित करता है जिन्हें आमतौर पर *input IDs* कहा जाता है। प्रत्येक अनुक्रम अब संख्याओं की एक सूची है! परिणामी आउटपुट है:

```py no-format
encoded_sequences = [
    [101, 7592, 999, 102],
    [101, 4658, 1012, 102],
    [101, 3835, 999, 102],
]
```

यह एन्कोडेड अनुक्रमों की एक सूची है: सूचियों की एक सूची। टेंसर केवल आयताकार आकार स्वीकार करते हैं (मैट्रिसेस सोचें)। यह "ऐरे" पहले से ही आयताकार आकार का है, इसलिए इसे टेंसर में बदलना आसान है:

{#if fw === 'pt'}
```py
import torch

model_inputs = torch.tensor(encoded_sequences)
```
{:else}
```py
import tensorflow as tf

model_inputs = tf.constant(encoded_sequences)
```
{/if}

### मॉडल के इनपुट के रूप में टेंसर का उपयोग करना

मॉडल के साथ टेंसर का उपयोग करना बेहद सरल है - हम केवल इनपुट के साथ मॉडल को कॉल करते हैं:

```py
output = model(model_inputs)
```

जबकि मॉडल कई अलग-अलग तर्कों को स्वीकार करता है, केवल इनपुट आईडी आवश्यक हैं। हम बताएंगे कि अन्य तर्क क्या करते हैं और जब बाद में उनकी आवश्यकता होती है, लेकिन पहले हमें उन टोकननाइज़रों पर करीब से नज़र डालने की ज़रूरत है जो उन इनपुट्स का निर्माण करते हैं जिन्हें एक ट्रांसफॉर्मर मॉडल समझ सकता है।
